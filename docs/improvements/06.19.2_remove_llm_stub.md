# Удаление заглушки LLM из IdentityService

## Цель
Реализовать полноценную работу IdentityService c LLM без промежуточной заглушки.

## Контекст
Сервис IdentityService использует LLM для уточнения совпадений сущностей. Сейчас по умолчанию передаётся лямбда `{"action": "new"}`, которая служит заглушкой и не позволяет получить реальные решения от модели. В каталоге `app/core/identity/prompts` сохранены старые промпты, использовавшиеся до рефакторинга.

## Мотивация
Устранение заглушки повысит качество распознавания имён и избавит от ручного вмешательства. Сервис будет готов к использованию с реальной моделью сразу после инициализации.

## Польза
После доработки при отсутствии переданного LLM будет создан стандартный экземпляр модели, способный обрабатывать промпт `verify_alias_llm.j2`. Код станет чище, а результаты разрешения сущностей точнее.

## Планируемые изменения
1. В `get_identity_service_sync` убрать лямбда‑заглушку. Если LLM не передан, инициализировать модель по умолчанию (например, `ChatOpenAI` из `langchain`).
2. Удалить fallback в `_llm_disambiguate_sync`, который напрямую вызывает `self._llm`.
3. Расширить `LLMDecision` значением `"skip"` и обрабатывать его в `_resolve_single_sync` — если ответ `skip`, нужно вернуть исходное значение без задачи на добавление алиаса.
4. Обновить функции `_llm_disambiguate` и `_llm_disambiguate_sync` так, чтобы они всегда использовали общий хелпер `call_llm_with_model`.
5. Использовать шаблон `verify_alias_llm.j2` из `app/core/identity/prompts` для построения промпта. Рассмотреть его улучшения: чётко описать действия (`use`, `new`, `skip`), подчеркнуть языковую инвариантность ответа и необходимость строгого JSON‑формата.
6. Дополнить тесты, проверяющие отсутствие заглушки и корректную работу с настоящей моделью (можно использовать `FakeListLLM`).

### Промпт `verify_alias_llm.j2`
Шаблон содержит подробное описание задачи для модели:

- объясняет различия между действиями `use`, `new` и `skip`;
- приводит примеры форматирования единого JSON-ответа;
- подчёркивает отсутствие лишнего текста в выводе и сохранение исходного языка.

Эти указания гарантируют, что модель вернёт строго валидный JSON и не будет переводить имена или комментарии. Поэтому промпт подходит для автоматизированного использования без дополнительной пост‑обработки. При необходимости его можно дополнить примерами «плохих» имён, чтобы повысить долю ответов `skip`.

## Локация
- `app/services/identity_service.py` — функции `get_identity_service_sync`, `_llm_disambiguate_sync` и `_llm_disambiguate`.
- Тесты: `app/tests/unit/services/identity_service/test_identity_service.py` и интеграционные тесты в `app/tests/integration/services/identity_service`.

## Альтернативы
Оставить заглушку и передавать рабочую модель при создании сервиса, но это требует больше кода от пользователя и ведёт к ошибкам.

## Деплой
Дополнительных действий не требуется, так как настройки LLM уже есть в конфигурации приложения.

## Проверка
- Запустить `pytest -q` — должны пройти все существующие тесты и новые тесты для LLM.
- Убедиться, что IdentityService инициализирует модель и использует промпт из `app/core/identity/prompts`.

## Меры на будущее
- Добавить документацию по подключению собственной LLM.
- Поддерживать тесты, покрывающие вызовы LLM, чтобы изменение интерфейса не привело к регрессии.

## Отчёт
После внедрения сервис будет сразу готов к работе с LLM без ручной передачи функции-заглушки.
